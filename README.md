# 🎙️ Speech Emotion Recognition Using LSTM and MFCCs

This project implements a deep learning model to classify human emotions from speech audio using MFCC (Mel Frequency Cepstral Coefficients) and LSTM (Long Short-Term Memory) networks.

---

## 🔍 Overview

The objective is to detect emotions such as:

- 😃 Happy
- 😢 Sad
- 😠 Angry
- 😨 Fearful
- 😐 Neutral
- 😖 Disgust
- 😲 Surprise

from raw `.wav` audio files.

This project includes:
- MFCC feature extraction using Librosa
- A deep LSTM-based model trained on the TESS dataset
- Achieved **99.6% validation accuracy**
- A Tkinter-based GUI to test real-time emotion predictions from audio input

---


## 🚀 Features

- 🔊 Converts audio speech to MFCCs
- 🧠 Uses LSTM for emotion classification
- 📊 Trained on 5600 samples from the TESS dataset
- 🖥️ Desktop GUI for real-time emotion detection

---

## 🧰 Tech Stack

- **Languages:** Python  
- **Libraries:** TensorFlow/Keras, NumPy, Librosa, Scikit-learn, Matplotlib  
- **Tools:** Jupyter Notebook, Tkinter

---
## Dataset
[Toronto Emotional Speech Set (TESS)](https://utoronto.scholaris.ca/collections/036db644-9790-4ed0-90cc-be1dfb8a4b66)



