# ğŸ™ï¸ Speech Emotion Recognition Using LSTM and MFCCs

This project implements a deep learning model to classify human emotions from speech audio using MFCC (Mel Frequency Cepstral Coefficients) and LSTM (Long Short-Term Memory) networks.

---

## ğŸ” Overview

The objective is to detect emotions such as:

- ğŸ˜ƒ Happy
- ğŸ˜¢ Sad
- ğŸ˜  Angry
- ğŸ˜¨ Fearful
- ğŸ˜ Neutral
- ğŸ˜– Disgust
- ğŸ˜² Surprise

from raw `.wav` audio files.

This project includes:
- MFCC feature extraction using Librosa
- A deep LSTM-based model trained on the TESS dataset
- Achieved **99.6% validation accuracy**
- A Tkinter-based GUI to test real-time emotion predictions from audio input

---


## ğŸš€ Features

- ğŸ”Š Converts audio speech to MFCCs
- ğŸ§  Uses LSTM for emotion classification
- ğŸ“Š Trained on 5600 samples from the TESS dataset
- ğŸ–¥ï¸ Desktop GUI for real-time emotion detection

---

## ğŸ§° Tech Stack

- **Languages:** Python  
- **Libraries:** TensorFlow/Keras, NumPy, Librosa, Scikit-learn, Matplotlib  
- **Tools:** Jupyter Notebook, Tkinter

---
## Dataset
[Toronto Emotional Speech Set (TESS)](https://utoronto.scholaris.ca/collections/036db644-9790-4ed0-90cc-be1dfb8a4b66)



